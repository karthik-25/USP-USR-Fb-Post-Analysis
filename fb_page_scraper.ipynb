{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import facebook\n",
    "import json\n",
    "import pprint\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "from tqdm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Format datetime from raw date to python datetime object, convert from UTC to UTC+8 (Local time)\n",
    "def format_datetime(raw_datetime):\n",
    "    newdatetime = datetime.strptime(raw_datetime, \"%Y-%m-%dT%H:%M:%S+0000\")\n",
    "    \n",
    "    timestamp = calendar.timegm(newdatetime.timetuple())\n",
    "    local_dt = datetime.fromtimestamp(timestamp)\n",
    "    assert newdatetime.resolution >= timedelta(microseconds=1)\n",
    "    return local_dt.replace(microsecond=newdatetime.microsecond)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Facebook access token hardcoded. Get this from Facebook Graph Explorer\n",
    "ACCESS_TOKEN = \"\"\n",
    "\n",
    "# Earliest create_date desired. Posts that go back to this date will be scraped.\n",
    "END_DATE = datetime.strptime(\"2014-01-01 00:00:00\", \"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Making API call to get all posts for given ID (in this case, politician's page)\n",
    "graph = facebook.GraphAPI(access_token=ACCESS_TOKEN, version='2.7')\n",
    "\n",
    "# Edit\"id\" below to specify page to scrape\n",
    "#all_posts = graph.get_object(id='leehsienloong/posts?limit=100')\n",
    "all_posts = graph.get_object(id='k.shanmugam.page/posts?limit=100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a list of post IDs from the JSON data (returned from API call)  \n",
    "post_id_list= []\n",
    "\n",
    "for post in all_posts[\"data\"]:\n",
    "    post_id_list.append(post[\"id\"])\n",
    "    \n",
    "cur_date = format_datetime(all_posts[\"data\"][99][\"created_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of posts: 850\n",
      "Date of oldest post: 2014-01-18 00:17:07\n"
     ]
    }
   ],
   "source": [
    "# Each API call returns a max of 100 posts. Iterate through \"next page\" link at end of every API call results page. \n",
    "# Stop when desired END DATE reached.\n",
    "\n",
    "while cur_date > END_DATE:\n",
    "    raw_next_page_url = all_posts[\"paging\"][\"next\"]\n",
    "    next_page_url = raw_next_page_url[raw_next_page_url.find(\"v2.7/\")+len(\"v2.7/\"):]\n",
    "    \n",
    "    all_posts = graph.get_object(id=next_page_url)\n",
    "    for post in all_posts[\"data\"]:\n",
    "        cur_date = format_datetime(post[\"created_time\"])\n",
    "        if cur_date > END_DATE:\n",
    "            post_id_list.append(post[\"id\"])\n",
    "\n",
    "print \"No. of posts:\",len(post_id_list)\n",
    "print \"Date of oldest post:\",format_datetime(all_posts[\"data\"][29][\"created_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [04:15<00:00,  3.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loop through each post, retrieve relevant post information (message, likes count, reactions count, comments count)\n",
    "# Store data in list. Each element is still in JSON format.\n",
    "post_desc_list = []\n",
    "fields_required = \"created_time, message, shares, comments.limit(0).summary(true), likes.limit(0).summary(true),reactions.type(LOVE).limit(0).summary(total_count).as(reactions_love),reactions.type(WOW).limit(0).summary(total_count).as(reactions_wow),reactions.type(HAHA).limit(0).summary(total_count).as(reactions_haha),reactions.type(SAD).limit(0).summary(total_count).as(reactions_sad),reactions.type(ANGRY).limit(0).summary(total_count).as(reactions_angry)\"\n",
    "\n",
    "for ids in tqdm(post_id_list):\n",
    "    post_desc_list.append(graph.get_object(id=ids, fields=fields_required))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850 posts' data stored in CSV successfully\n"
     ]
    }
   ],
   "source": [
    "# Loop through post_desc_list, Each interation, retrieve relevant info, create row and write into csv file\n",
    "\n",
    "headers = [\"No.\",\"created_time\",\"status\",\"comments_count\",\"shares_count\",\"likes_count\",\"angry_count\",\"haha_count\",\"love_count\",\"sad_count\",\"wow_count\"]\n",
    "\n",
    "with open('datafile_k_shanmugam.csv', 'wb') as f:\n",
    "    w = csv.writer(f, delimiter=\",\")\n",
    "    w.writerow(headers)\n",
    "    count = 1\n",
    "    for post_desc in post_desc_list:\n",
    "        \n",
    "        created_time = format_datetime(str(post_desc[\"created_time\"]))\n",
    "        \n",
    "        try: \n",
    "            status_str = str(post_desc[\"message\"].encode('utf8'))\n",
    "        except KeyError:\n",
    "            status_str = \"\"\n",
    "        \n",
    "        comments_count = str(post_desc[\"comments\"][\"summary\"][\"total_count\"])\n",
    "        \n",
    "        try:\n",
    "            shares_count = str(post_desc[\"shares\"][\"count\"])\n",
    "        except KeyError:\n",
    "            shares_count = 0\n",
    "        \n",
    "        likes_count = str(post_desc[\"likes\"][\"summary\"][\"total_count\"])\n",
    "        angry_count = str(post_desc[\"reactions_angry\"][\"summary\"][\"total_count\"])\n",
    "        haha_count = str(post_desc[\"reactions_haha\"][\"summary\"][\"total_count\"])\n",
    "        love_count = str(post_desc[\"reactions_love\"][\"summary\"][\"total_count\"])\n",
    "        sad_count = str(post_desc[\"reactions_sad\"][\"summary\"][\"total_count\"])\n",
    "        wow_count = str(post_desc[\"reactions_wow\"][\"summary\"][\"total_count\"])\n",
    "        \n",
    "        row_string = [count,created_time,status_str,comments_count,shares_count,likes_count,angry_count,haha_count,love_count,sad_count,wow_count]\n",
    "        w.writerow(row_string)\n",
    "        count += 1\n",
    "\n",
    "print count-1,\"posts' data stored in CSV successfully\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
